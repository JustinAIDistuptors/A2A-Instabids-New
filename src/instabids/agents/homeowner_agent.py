"""HomeownerAgent: multimodal concierge for homeowners."""from __future__ import annotationsfrom pathlib import Pathfrom typing import Any, Dict, List, Optionalfrom google.adk import LlmAgent, enable_tracingfrom google.adk.messages import UserMessagefrom instabids.tools import supabase_tools, openai_vision_toolfrom instabids.tools.stt_tool import speech_to_textfrom instabids.a2a_comm import send_envelopefrom memory.persistent_memory import PersistentMemoryfrom memory.conversation_state import ConversationStateimport loggingfrom instabids.data import project_repo as repofrom .job_classifier import classify_job# enable stdout tracing for dev envsenable_tracing("stdout")# Set up logginglogger = logging.getLogger(__name__)SYSTEM_PROMPT = (    "You are HomeownerAgent, a long‑term concierge for homeowners. "    "Given images, voice/text, or forms, classify the project, ask clarifying "    "questions, persist structured data to Supabase, and emit a 'project.created' "    "A2A envelope once you have enough details. Keep memory of preferences and "    "prior jobs for each user.")class HomeownerAgent(LlmAgent):    """Concrete ADK agent with multimodal intake and memory."""    def __init__(self, memory: Optional[PersistentMemory] = None):        super().__init__(name="HomeownerAgent", tools=[*supabase_tools, openai_vision_tool], system_prompt=SYSTEM_PROMPT, memory=memory or PersistentMemory())        async def gather_project_info(self, user_id: str, description: Optional[str] = None, form_payload: Optional[Dict[str, Any]] = None, project_id: Optional[str] = None) -> Dict[str, Any]:        """        Gather project information through slot-filling.                Args:            user_id: User ID for preference lookup/storage            description: Optional initial project description            form_payload: Optional form data            project_id: Optional project ID for context                        Returns:            Dict with project info or next question        """        # Initialize or get state        state_key = f"project_info:{user_id}"        state = self.memory.get(state_key, ConversationState())                # Add new input if provided        if description:            state.add_user_input(description)                    # Determine next action based on state        if state.is_complete():            # We have all required information            return {                "need_more": False,                "project": state.get_slots()            }        else:            # Need more info - determine what to ask next            next_slot = state.get_next_slot()            question = state.get_question_for_slot(next_slot)                        # Save state            self.memory.set(state_key, state)                        return {                "need_more": True,                "next_slot": next_slot,                 "question": question,                "project": state.get_slots()            }        async def process_input(        self,         user_id: str,         description: Optional[str] = None,         form_payload: Optional[Dict[str, Any]] = None,        base64_audio: Optional[str] = None,        project_id: Optional[str] = None,        image_paths: List[Path] | None = None    ) -> Dict[str, Any]:        """        Process user input from various sources (text, audio, form).                Args:            user_id: User ID for preference lookup/storage            description: Optional text description            form_payload: Optional form data            base64_audio: Optional base64-encoded audio            project_id: Optional project ID for context            image_paths: Optional paths to images                        Returns:            Response dict with next question or project info        """        # Process audio input if provided        if base64_audio:            transcript = await speech_to_text(base64_audio)            if transcript:                description = transcript                logger.info(f"Processed audio input: {description[:50]}...")            else:                logger.warning("Audio transcription failed or was rejected")                return {"error": "Could not understand audio. Please try again or type your request."}                # Process form input if provided        if form_payload:            # TODO: Handle form data            pass                # Process images if provided        vision_context: dict[str, Any] = {}        if image_paths:            vision_context = await self._process_images(image_paths)        # Build full prompt for ADK if using original process method        if description or vision_context:            content_parts = []            if description:                content_parts.append(description)            if vision_context:                content_parts.append(f"Vision context: {vision_context}")            user_msg = UserMessage("\n".join(content_parts))            response = await self.chat(user_msg)            # Classify job type & urgency (simple rule‑based for v1)            classification = classify_job(description or "", vision_context)            # Persist project            project_id = await self.create_project(                description=description or "(image-only)",                category=classification["category"],                urgency=classification["urgency"],                user_id=user_id,                vision_context=vision_context,            )            return {                "agent_response": response.content,                "project_id": project_id,                **classification,            }                    # Otherwise, gather project information using slot-based approach        return await self.gather_project_info(user_id, description, form_payload, project_id)    async def answer_question(self, question: str, context: Optional[Dict[str, Any]] = None) -> str:        """        Answer a homeowner's question about their project.                Args:            question: The user's question            context: Optional context like project history                        Returns:            Agent's response        """        # TODO: Implement answer_question        return "I'll need to look that up for you."        async def create_project(self, description: str, images: Optional[List[Dict[str, Any]]] = None,                             category: Optional[str] = None, urgency: Optional[str] = None,                            user_id: Optional[str] = None, vision_context: Optional[Dict[str, Any]] = None) -> str:        """Create a project in the database."""        # Prepare project row        cls = classify_job(description, vision_context or {})        row = {            "description": description,            "homeowner_id": user_id,            "category": category or cls["category"],            "urgency": urgency or cls["urgency"],            "confidence": cls["confidence"],        }        try:            pid = repo.save_project(row)            if images:                repo.save_project_photos(pid, images)        except Exception as err:            logger.error(f"Failed to save project: {err}")            raise                    # --- emit A2A envelope -----------------------------------------        payload = {"project_id": pid, "homeowner_id": row["homeowner_id"]}        send_envelope("project.created", payload, "homeowner_agent")        return pid    # ─────────────────────────────────────────────────────────────────────────────────    # Internal helpers    # ─────────────────────────────────────────────────────────────────────────────────    async def _process_images(self, image_paths: List[Path]) -> dict[str, Any]:        """Call the vision tool and return parsed context."""        context: dict[str, Any] = {}        for p in image_paths:            result = await openai_vision_tool.call(image_path=str(p))            context[p.name] = result        return context